---
title: 算法之哈希算法
date: 2019-09-03 19:47:40
keywords: 哈希算法
description: 如何实现一个通用的、高性能的排序函数?
categories: 
  - 算法
tags:
  - 排序
comments: false
---

# 哈希算法

**<u>将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。</u>** 

优秀哈希算法的要求：

- 从哈希值不能反向推导出原始数据。（哈希算法也叫单向哈希算法）
- 对输入的数据比较敏感，哪怕原始数据只修改了一个bit,最后得到的哈希值也大不相同
- 散列冲突的概率要小，对于不同的原始数据，哈希值相同的概率非常小。
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速计算出哈希值。

以MD5为例：

> MD5("我今天讲哈希算法!") = 425f0d5a917188d2c3c3dc85b5e4f2cb
> MD5("我今天讲哈希算法") = a1fb91ac128e6aa37fe42c663971ac3d
>
> 看到计算结果差距很大。

# 应用一：安全加密

最常用于加密的 **哈希算法** 有：

- **MD5 (MD5 Message-Digest Algorithm, MD5消息摘要算法**。
- **DES (Data Encryption Standard, 数据加密标准**。
- **AES (Advanced Encryption Standard, 高级加密标准**。

安全加密有两点格外重要：

- 第一点是很难根据哈希值反向推导出原始数据，
- 第二点是散列冲突的概率要很小。

哈希算法理论上市无法做到完全不冲突！

这里就基于组合数学中一个非常基础的理论，鸽巢原理(也叫抽屉原理)。这个原理本身很简单，它是说，如果有10个鸽巢，有11只鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。

# 应用二：唯一标识

哈希算法可以用来作为某些场景的唯一标识。

比如：要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息(比如图片名称)来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。

> 我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字
> 节，然后将这300个字节放到一块，通过哈希算法(比如MD5)，得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库
> 中，这样就可以减少很多工作量。

# 应用三：数据校验

电驴BT文件校验。

> 我们通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

# 应用四：散列函数

散列函数也是哈希算法的一种应用。

- 相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多；
- 散列函数对于散列算法计算得到的值，是否能反向解密也并不关心；
- 散列函数用的散列算法一般都比较简单，比较追求效率。

**以下是跟分布式系统有关：**

# 应用五：负载均衡

负载均衡算法有很多，比如轮询、随机、加权轮询等；**如何才能实现一个会话粘滞(session sticky)的负载均衡算法呢?也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上？**

**方法一：**

维护一张映射关系表，这张表的内容是客户端IP地址或者会话ID与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。

缺点：

- 如果客户端很多，映射表可能会很大，比较浪费内存空间;
- 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大;

**方法二：**

借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端**IP**地址或者会话**ID**计算哈希值，将取得的哈希值与服务器列表的大小进 行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。 

# 应用六：数据分片

## 如何统计**“**搜索关键词**”**出现的次数?

假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢?

**难点：**

- 一个是搜索日志很大，没办法放到一台机器的内存中
- 二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。 

**解决：**

我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的:为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编 号。 

这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。

实际上，这里的处理过程也是MapReduce的基本设计思想 。

## 如何快速判断图片是否在图库中?

假设现在我们的图库中有1亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。

**解决：**

我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯 一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。 

当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编 号k的机器构建的散列表中查找。 

# 应用七:分布式存储

现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。
该如何决定将哪个数据放到哪个机器上呢?我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。

但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了。因为，这里并不是简单地加个机器就可以 了。 

原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配 到2号这台机器上了。 

![image](https://raw.githubusercontent.com/HaviLee/Blog-Images/master/高手/09032113.png)

因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。
所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了。

假设我们有k个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成m个小区间(m远大于k)，每个机器负责m/k个小区间。当有新机器加入的时候， 我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。 

